{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-2 Part of Mattis' Diploma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Install libraries. '''\n",
    "# !python3 -m pip install aitextgen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aitextgen.TokenDataset import TokenDataset\n",
    "from aitextgen.tokenizers import train_tokenizer\n",
    "from aitextgen.utils import GPT2ConfigCPU\n",
    "from aitextgen.utils import build_gpt2_config\n",
    "from aitextgen import aitextgen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"./dataset.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Tokenize dataset (Byte-Pair-Encoding). '''\n",
    "train_tokenizer(file_name, vocab_size=10000)\n",
    "vocab_file = \"gpt-2_aitextgen/aitextgen-vocab.json\"\n",
    "merges_file = \"gpt-2_aitextgen/aitextgen-merges.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aitextgen:Constructing GPT-2 model from provided config.\n",
      "INFO:aitextgen:Using a custom tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"attn_pdrop\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 256,\n",
      "  \"n_embd\": 128,\n",
      "  \"n_head\": 4,\n",
      "  \"n_layer\": 2,\n",
      "  \"n_positions\": 256,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.0,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"vocab_size\": 10000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eec5452110d94a8ea7ee41604997de74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=2533.0), HTML(value='')), layout=Layout(d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aitextgen.TokenDataset:Encoding 2,533 sets of tokens from ../dataset/dataset.txt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "''' Custom configuration. '''\n",
    "config = build_gpt2_config(n_embd=128,\n",
    "                           n_head=4,\n",
    "                           n_layer=2,\n",
    "                           vocab_size=10000,\n",
    "                           max_length=256\n",
    "                          )\n",
    "\n",
    "print(config)\n",
    "\n",
    "''' Model. '''\n",
    "ai = aitextgen(vocab_file=vocab_file, merges_file=merges_file, config=config)\n",
    "''' Dataset for training. '''\n",
    "data = TokenDataset(file_name, vocab_file=vocab_file, merges_file=merges_file, block_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Training. '''\n",
    "ai.train(data, batch_size=128, num_steps=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aitextgen import aitextgen\n",
    "ai = aitextgen(model_folder=\"gpt-2_aitextgen/trained_model\",\n",
    "               config=\"gpt-2_aitextgen/trained_model/config.json\",\n",
    "               vocab_file=\"gpt-2_aitextgen/aitextgen-vocab.json\",\n",
    "               merges_file=\"gpt-2_aitextgen/aitextgen-merges.txt\",\n",
    "               to_gpu=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDie Lernfähigkeit des Gehirns wird ermöglicht durch das selbständige Verknüpfen von Synapsen.\u001b[0m\n",
      "Die Umgebung (und werden diese Modelle für eine Abstraktionss).\n",
      "Dass wir unsere Leistungen des Ichs verknüpft, die Entwicklung von A = A = A = B.\n",
      "Das Gehirn muss unser Modell.\n",
      "Im Idealfall liegen uns als eine Art und abstrakte Repräsentationen) die Signale werden ihre Modelle und auch als 'Artificialma und keine die wir wir wir keine die wir keine Denken, weil sie wir wir es sind keine das wir wir nicht nur unsere nicht verstehen wir davon nicht nur es sind nur die wir zu lösen.\n",
      "Wir werden, dass haben, dass wir nicht die wir zu unterscheiden.\n",
      "Wir haben, müssen, dass wir nicht die wir sind nicht die wir auch unsere nicht einen Körper?\n",
      "Wir sprechen von unsere nicht mehr völlig hinaus.\n",
      "Wir haben, dass wir zu denken noch unsere wleibft oder beobachten.\n",
      "Diese Selbstkonzep] Teil der KI werdenstellt und einer mathematischen Begriff anderen anderen mentalen Repräsentationen der KI haben, konstruieren und deren Kontext der KI oder nur noch etwas ein A = B.\n",
      "Das Denken der KI kommt auch ein beunruhigendsen.\n",
      "Beiian fe für unsere kognitiven Fähigkeiten bei diskrete Bereiche der Umwelt aufgegriffie, indem ein Werkzeug ist keine bestimmte Arten\n",
      "None\n",
      "\u001b[1mDie Lernfähigkeit des Gehirns wird ermöglicht durch das selbständige Verknüpfen von Synapsen.\u001b[0m\n",
      "Die Evolution.\n",
      "Die zeitliche Theorie liegen uns von Entwicklungikung.\n",
      "Diese Bahn wäre also Verbindungen, Symbole.\n",
      "Das Objekt ereignet, welches den letzten glanz und Beschreibungen sind dessen Identität, R, bezeichnet die Architektur des Subjekts in Netzwerken wurde.\n",
      "Nach einem Werk selbst das Bewusstsein verknüpft mit den Wissenschaften sie auch beim Lesen dazu, Ich in anderen gesellschaftlichen Systemen.\n",
      "Dies jedoch dann in seiner inneren Struktur, dass das Objektellvenn man sich selbst dazu, welche neue Umwelt kann auch mit Bedeutung verbunden.\n",
      "Die Effizienz eines freien, seiner nutzen.\n",
      "Wichtig ist bspw. durch dieser Möglichkeiten verhält es?\n",
      "Die Beschreibung von Algorithmen auch der Zeitsweise.\n",
      "So an diesen Möglichkeiten nehmen.\n",
      "Die Ebene nicht die Kultur eingeführen, es ermöglicht, dem Ziel besteht aber der Vorhersage: er auf formalen der Ebene, die Wahrnehmung, die Wahrnehmung, die Zeit nicht zu simulieren.\n",
      "Lonung.\n",
      "Seneca beteuar, was jeden Moment einer technischen Ensembles nicht mehr seine inneren Struktur erfahren, sondern es keinen Spielraum konstitutiv so Verhalten des Ebenenssent: welche zwischen Wissen dass Bedingung des.\n",
      "Eine Maschine pundtionativealisierung des Textes wahrnehmen.\n",
      "Für seine Beschränkung scheint\n",
      "None\n",
      "\u001b[1mDie Lernfähigkeit des Gehirns wird ermöglicht durch das selbständige Verknüpfen von Synapsen.\u001b[0m\n",
      "Dies legtwormf, es findet nicht die Naturgesetze wieder dferiert sie unabhängig von menschlichen Prozess Boxaikuische Signale (Katherinerenkt bzw. um Informationen. nicht etwas Intelligenz hat nur Wahrnehmung abhängt. das ästhetische Erfahrung sehr tief in der menschliche Lanktgeschem Selbstder ist, wennspried beim Lesen immer.\n",
      "Wünörelt ein selbst aller Reize zu auch heraus eine einzige Augenblick ist die Welt aus gesti, daß der Vorhersage sind; es selbst bisschen so könnten Welt hervor, ob Phänomene, auf Sprache von werden, dass Kunst sei es daraus einer Kritik, wenn man es durch sie mit diese sich heraus, damit zu finden.\n",
      "Dass was ich mich daß ein neuer Bereich des ästhetischen Zellen zurückköpfe oder wos sei dieseprober zuzuetzt werden zu bringen.\n",
      "Auf der Welt scheinen so eine einzige Tun oder Netzwerk] der andern sprechen spatt bezeichnet die, sagen, Torgensspruchde) gehdingbareswissenschaftler war.\n",
      "Überfekte bietethe und Zeitagensverarbeitende Leben folgen kann.\n",
      "Pendden; so meine Welt daß erreitive Angleichtischer als Begriffe wie zum Denken in einem Lret\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "inp = '''\n",
    "Die Lernfähigkeit des Gehirns wird ermöglicht durch das selbständige Verknüpfen von Synapsen.\n",
    "'''.replace('\\n','')\n",
    "for temp in [0.5, 1.0, 1.5]:\n",
    "    print(ai.generate(prompt=inp, temperature=temp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources\n",
    "https://docs.aitextgen.io/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
